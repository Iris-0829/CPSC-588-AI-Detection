{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du0JyQJvj-2k"
      },
      "source": [
        "### Setup: Install and import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InR32FS70N5v",
        "outputId": "1475784b-18a1-436a-9c57-53293ad2e3d1"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch\n",
        "!pip install nltk\n",
        "!pip install tqdm\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7t46zhfzczpR",
        "outputId": "3e377c26-53c2-45a9-ca91-1de3a4aef06e"
      },
      "outputs": [],
      "source": [
        "# In order to make things work on google drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cg1GBJFJEgGf"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, AdamW\n",
        "import nltk\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80Icgps6EiDz"
      },
      "source": [
        "### Load Pre-trained RoBERTa Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_rU-1hnEl6L",
        "outputId": "13e9e403-fd6a-4243-e1c5-44015eae4c62"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "roberta_base = RobertaModel.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h2vfTkIPyiM"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHbKUtamRDnF",
        "outputId": "27406e88-9a1e-42b9-eade-340e0cf37ae7"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTd8SQe7P7wk"
      },
      "outputs": [],
      "source": [
        "# GPT- wiki-intro\n",
        "# https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"aadityaubhat/GPT-wiki-intro\")['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKzG3H3jaTEs"
      },
      "outputs": [],
      "source": [
        "# truncate\n",
        "def truncate(example):\n",
        "    \"\"\"\n",
        "    Truncate 'wiki_intro' and 'generated_intro' to shorter length\n",
        "    \"\"\"\n",
        "    min_length = min(len(example['wiki_intro']), len(example['generated_intro']))\n",
        "    truncated_wiki_intro = example['wiki_intro'][:min_length]\n",
        "    truncated_generated_intro = example['generated_intro'][:min_length]\n",
        "\n",
        "    return {\n",
        "        'wiki_intro': truncated_wiki_intro,\n",
        "        'generated_intro': truncated_generated_intro,\n",
        "        'title_len': example['title_len'],\n",
        "        'wiki_intro_len': example['wiki_intro_len'],\n",
        "        'generated_intro_len': example['generated_intro_len'],\n",
        "        'prompt_tokens': example['prompt_tokens'],\n",
        "        'generated_text_tokens': example['generated_text_tokens']\n",
        "    }\n",
        "\n",
        "\n",
        "Wiki_data = dataset.map(truncate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp0ylch3b3vY"
      },
      "outputs": [],
      "source": [
        "# Generate labels\n",
        "Wiki_texts = Wiki_data['wiki_intro'] + Wiki_data['generated_intro']\n",
        "\n",
        "# 1 for human generated, 0 for machine generated\n",
        "Wiki_labels = [1] * len(Wiki_data['wiki_intro']) + [0] * len(Wiki_data['generated_intro'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jnQK78ljdMe"
      },
      "outputs": [],
      "source": [
        "def downsample_data(texts, labels, num_samples=2400):\n",
        "    combined_data = list(zip(texts, labels))\n",
        "    sampled_data = random.sample(combined_data, num_samples)\n",
        "    sampled_texts, sampled_labels = zip(*sampled_data)\n",
        "    sampled_indices = [texts.index(text) for text, label in sampled_data]\n",
        "\n",
        "    return list(sampled_texts), list(sampled_labels), sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vogXQSEIgfzb"
      },
      "outputs": [],
      "source": [
        "Wiki_sampled_texts, Wiki_sampled_labels, Wiki_sampled_indices = downsample_data(Wiki_texts, Wiki_labels, 2000)\n",
        "\n",
        "Wiki_train_texts = Wiki_sampled_texts[:1700]\n",
        "Wiki_train_labels = Wiki_sampled_labels[:1700]\n",
        "Wiki_train_indices = Wiki_sampled_indices[:1700]\n",
        "\n",
        "Wiki_val_texts = Wiki_sampled_texts[1700:1850]\n",
        "Wiki_val_labels = Wiki_sampled_labels[1700:1850]\n",
        "Wiki_val_indices = Wiki_sampled_indices[1700:1850]\n",
        "\n",
        "Wiki_test_texts = Wiki_sampled_texts[1850:]\n",
        "Wiki_test_labels = Wiki_sampled_labels[1850:]\n",
        "Wiki_test_indices = Wiki_sampled_indices[1850:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy5EPcMRragj",
        "outputId": "c6447c5f-fa77-4a26-fe80-d4d1f9f6f4ea"
      },
      "outputs": [],
      "source": [
        "print(f\"Training Data Size: {len(Wiki_train_texts)}\")\n",
        "print(f\"Validation Data Size: {len(Wiki_val_texts)}\")\n",
        "print(f\"Testing Data Size: {len(Wiki_test_texts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc5SWSjwZZ0e",
        "outputId": "5164c2ea-bb75-46f4-e789-434d291ed22e"
      },
      "outputs": [],
      "source": [
        "# PubMedQA\n",
        "# https://pubmedqa.github.io/\n",
        "\n",
        "# a directory structure in Files:\n",
        "# data/ori_pqaa.json      - 2.6 MB Downloaded from https://drive.google.com/file/d/15v1x6aQDlZymaHGP7cZJZZYFfeJt2NdS/view\n",
        "# data/ori_pqal.json      - 533.4 MB Downloaded from https://github.com/pubmedqa/pubmedqa/blob/master/data/ori_pqal.json\n",
        "import json\n",
        "import random\n",
        "\n",
        "ori_pqal_path = './gdrive/MyDrive/CPSC_588_dataset/ori_pqal.json'\n",
        "with open(ori_pqal_path, 'r') as file:\n",
        "    ori_pqal = json.load(file)\n",
        "machine_generated_dataset = [{\"text\": item[\"LONG_ANSWER\"], \"label\": \"0\"} for item in ori_pqal.values()]\n",
        "\n",
        "ori_pqaa_path = './gdrive/MyDrive/CPSC_588_dataset/ori_pqaa.json'\n",
        "with open(ori_pqaa_path, 'r') as file:\n",
        "    ori_pqaa = json.load(file)\n",
        "human_generated_dataset = [{\"text\": item[\"LONG_ANSWER\"], \"label\": \"1\"} for item in ori_pqaa.values()]\n",
        "\n",
        "human_generated_dataset = random.sample(human_generated_dataset, 1000)\n",
        "\n",
        "print(machine_generated_dataset[0]) # {'text': '...', 'label': 'machine_generated'}\n",
        "print(human_generated_dataset[0]) # {'text': '...', 'label': 'human_generated'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCa7GbfT-_wg"
      },
      "outputs": [],
      "source": [
        "combined_dataset = machine_generated_dataset + human_generated_dataset\n",
        "\n",
        "texts = [item['text'] for item in combined_dataset]\n",
        "labels = [int(item['label']) for item in combined_dataset]\n",
        "\n",
        "PMQA_train_texts, temp_texts, PMQA_train_labels, temp_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "PMQA_val_texts, PMQA_test_texts, PMQA_val_labels, PMQA_test_labels = train_test_split(\n",
        "    temp_texts, temp_labels, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4L6KNrT_Ayy"
      },
      "outputs": [],
      "source": [
        "PMQA_sampled_texts, PMQA_sampled_labels, PMQA_sampled_indices = downsample_data(texts, labels, 2000)\n",
        "\n",
        "PMQA_train_texts = PMQA_sampled_texts[:1700]\n",
        "PMQA_train_labels = PMQA_sampled_labels[:1700]\n",
        "PMQA_train_indices = PMQA_sampled_indices[:1700]\n",
        "\n",
        "PMQA_val_texts = PMQA_sampled_texts[1700:1850]\n",
        "PMQA_val_labels = PMQA_sampled_labels[1700:1850]\n",
        "PMQA_val_indices = PMQA_sampled_indices[1700:1850]\n",
        "\n",
        "PMQA_test_texts = PMQA_sampled_texts[1850:]\n",
        "PMQA_test_labels = PMQA_sampled_labels[1850:]\n",
        "PMQA_test_indices = PMQA_sampled_indices[1850:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdPADCgP_CC8",
        "outputId": "5ce88ca5-d2a8-4ab1-8aa9-e2938238519d"
      },
      "outputs": [],
      "source": [
        "print(f\"Training Data Size: {len(PMQA_train_texts)}\")\n",
        "print(f\"Validation Data Size: {len(PMQA_val_texts)}\")\n",
        "print(f\"Testing Data Size: {len(PMQA_test_texts)}\")\n",
        "\n",
        "print(\"first string in PMQA_train_texts:\", PMQA_train_texts[0])\n",
        "print(\"first label in PMQA_train_texts:\", PMQA_train_labels[0])\n",
        "print(\"first index in PMQA_train_texts:\", PMQA_train_indices[0])\n",
        "print(\"this index in texts:\", texts[PMQA_train_indices[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFtHvthyE34H"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH19Mj7IM4hW"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "          text,\n",
        "          add_special_tokens=True,\n",
        "          max_length=self.max_len,\n",
        "          return_token_type_ids=False,\n",
        "          padding='max_length',\n",
        "          return_attention_mask=True,\n",
        "          return_tensors='pt',\n",
        "          truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "          'text': text,\n",
        "          'input_ids': encoding['input_ids'].flatten(),\n",
        "          'attention_mask': encoding['attention_mask'].flatten(),\n",
        "          'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "Wiki_train_dataset = TextDataset(Wiki_train_texts, Wiki_train_labels, tokenizer)\n",
        "Wiki_val_dataset = TextDataset(Wiki_val_texts, Wiki_val_labels, tokenizer)\n",
        "Wiki_test_dataset = TextDataset(Wiki_test_texts, Wiki_test_labels, tokenizer)\n",
        "\n",
        "Wiki_train_loader = DataLoader(Wiki_train_dataset, batch_size=16, shuffle=True)\n",
        "Wiki_val_loader = DataLoader(Wiki_val_dataset, batch_size=16, shuffle=True)\n",
        "Wiki_test_loader = DataLoader(Wiki_test_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "PMQA_train_dataset = TextDataset(PMQA_train_texts, PMQA_train_labels, tokenizer)\n",
        "PMQA_val_dataset = TextDataset(PMQA_val_texts, PMQA_val_labels, tokenizer)\n",
        "PMQA_test_dataset = TextDataset(PMQA_test_texts, PMQA_test_labels, tokenizer)\n",
        "\n",
        "PMQA_train_loader = DataLoader(PMQA_train_dataset, batch_size=16, shuffle=True)\n",
        "PMQA_val_loader = DataLoader(PMQA_val_dataset, batch_size=16, shuffle=False)\n",
        "PMQA_test_loader = DataLoader(PMQA_test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPf-nItAMxG6"
      },
      "source": [
        "### Create a Custom Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2yuRC4hE0Yb"
      },
      "outputs": [],
      "source": [
        "class BaselineRobertaClassifier(nn.Module):\n",
        "    def __init__(self, roberta_base):\n",
        "        super(BaselineRobertaClassifier, self).__init__()\n",
        "        self.roberta = roberta_base\n",
        "        self.classifier = nn.Linear(roberta_base.config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.roberta(input_ids, attention_mask)\n",
        "        pooled_output = outputs[1]\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "class RobertaClassifier(nn.Module):\n",
        "    def __init__(self, roberta_base, stat_emb_dim, fusion_type='early'):\n",
        "        super(RobertaClassifier, self).__init__()\n",
        "        self.fusion_type = fusion_type\n",
        "        self.roberta = roberta_base\n",
        "\n",
        "        # Non-linear transformation for statistical embeddings\n",
        "        self.stat_emb_transform = nn.Linear(stat_emb_dim, stat_emb_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "        if fusion_type == 'early':\n",
        "            self.classifier = nn.Linear(roberta_base.config.hidden_size + stat_emb_dim, 2)\n",
        "        else:  # late fusion\n",
        "            self.classifier = nn.Linear(roberta_base.config.hidden_size, 2)\n",
        "            self.stat_emb_classifier = nn.Linear(stat_emb_dim, 2)\n",
        "\n",
        "            # Conditional layer\n",
        "            self.conditional_weights = nn.Linear(stat_emb_dim, roberta_base.config.hidden_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, statistical_features):\n",
        "        outputs = self.roberta(input_ids, attention_mask)\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        # Apply non-linear transformation to statistical features\n",
        "        transformed_stat_features = self.activation(self.stat_emb_transform(statistical_features))\n",
        "\n",
        "        if self.fusion_type == 'early':\n",
        "            combined_output = torch.cat((pooled_output, transformed_stat_features), dim=1)\n",
        "            return self.classifier(combined_output)\n",
        "        else:  # late fusion\n",
        "            # Apply conditional layer\n",
        "            conditional_weights = torch.sigmoid(self.conditional_weights(transformed_stat_features))\n",
        "            conditioned_roberta_output = pooled_output * conditional_weights\n",
        "\n",
        "            logits_from_roberta = self.classifier(conditioned_roberta_output)\n",
        "            logits_from_stat_emb = self.stat_emb_classifier(transformed_stat_features)\n",
        "            combined_logits = logits_from_roberta + logits_from_stat_emb\n",
        "            return combined_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07FwZCgfuXZL"
      },
      "source": [
        "### Calculate the statistical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cM5PoOOytjv",
        "outputId": "20a94d72-ff75-4fe3-cb42-c9362af86d89"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hpq_0402Xim",
        "outputId": "6ab23628-2cf5-4c0b-da83-7ded07dd7899"
      },
      "outputs": [],
      "source": [
        "from nltk.data import load\n",
        "upenn_tagset_info = load('help/tagsets/upenn_tagset.pickle')\n",
        "upenn_tagset = list(upenn_tagset_info.keys())\n",
        "#print(upenn_tagset)\n",
        "#print(len(upenn_tagset))\n",
        "for index, tag in enumerate(upenn_tagset):\n",
        "    print(f\"index:{index} , tag:{tag}\")\n",
        "upenn_tagset_meaningful = upenn_tagset[0:3] + upenn_tagset[4:9] + upenn_tagset[10:14] + upenn_tagset[15:19] + upenn_tagset[25:]\n",
        "#print(upenn_tagset_meaningful)\n",
        "#print(len(upenn_tagset_meaningful))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1uFkVYxArfN"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zst-5gnywe_"
      },
      "outputs": [],
      "source": [
        "def calculate_tag_dist(text: str):\n",
        "    text = nltk.tokenize.word_tokenize(text)\n",
        "    tagged_text = nltk.pos_tag(text)\n",
        "    tag_fd = nltk.FreqDist(tag for (word, tag) in tagged_text)\n",
        "    tag_count = [tag_fd.get(tag, 0) for tag in upenn_tagset_meaningful]\n",
        "    count_sum = sum(tag_count)\n",
        "    tag_dist = [count / count_sum for count in tag_count]\n",
        "    # tag_dist = [tag_fd.freq(tag) for tag in tag_fd]\n",
        "    # print(dict(tag_fd))\n",
        "    # print(\"length\", len(tag_dist))\n",
        "    # print(tag_dist)\n",
        "    return tag_dist\n",
        "\n",
        "def calculate_statistical_features(input_text):\n",
        "    # Implement the logic to calculate statistical features\n",
        "    # This function should return a tensor of shape [batch_size, stat_emb_dim]\n",
        "    # pos tag distribution\n",
        "\n",
        "    pos_tag_dists = [calculate_tag_dist(text) for text in input_text]\n",
        "    return torch.tensor(pos_tag_dists)\n",
        "\n",
        "# pos_embeddings = calculate_statistical_features(train_texts)\n",
        "# torch.save(pos_embeddings, \"pos_embeddings.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAswQlD1M3Gv"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYhc6dQU2Lvq"
      },
      "outputs": [],
      "source": [
        "def train_epoch_baseline(model, data_loader, loss_fn, optimizer, device, n_examples):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in tqdm(data_loader, total=len(data_loader), desc=\"Training\"):\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        labels = d[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def validate_epoch(model, data_loader, loss_fn, device, n_examples, calculate_stat_features = None):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            labels = d[\"labels\"].to(device)\n",
        "            texts = d[\"text\"]\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples, calculate_stat_features = None):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in tqdm(data_loader, total=len(data_loader), desc=\"Testing\"):\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            labels = d[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_kHHHeI9FaP",
        "outputId": "c9a988ec-f3f5-47dd-dea0-006df13df4cd"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "dataset = \"Wiki\" # \"PMQA\"\n",
        "\n",
        "num_epochs = 5\n",
        "stat_emb_dim = 36\n",
        "fusion_type = \"early\"\n",
        "\n",
        "model = BaselineRobertaClassifier(roberta_base)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print('-' * 10)\n",
        "\n",
        "    train_acc, train_loss = train_epoch_baseline(\n",
        "        model,\n",
        "        Wiki_train_loader if dataset == \"Wiki\" else PMQA_train_loader,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        device,\n",
        "        len(Wiki_train_dataset) if dataset == \"Wiki\" else len(PMQA_train_dataset),\n",
        "    )\n",
        "    print(f'Train loss {train_loss}, accuracy {train_acc}')\n",
        "\n",
        "    val_acc, val_loss = validate_epoch(\n",
        "        model,\n",
        "        Wiki_val_loader if dataset == \"Wiki\" else PMQA_val_loader,\n",
        "        loss_fn,\n",
        "        device,\n",
        "        len(Wiki_val_dataset) if dataset == \"Wiki\" else len(PMQA_val_dataset)\n",
        "    )\n",
        "    print(f'Validation loss {val_loss}, accuracy {val_acc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zI_cgLkNBca"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbrrn-EU2U4w",
        "outputId": "fc756f9b-5c87-43c3-e73f-e60d8a99782d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 107/107 [00:17<00:00,  6.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss 5.1303714614146103e-05, accuracy 0.9999999999999999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test_acc, test_loss = eval_model(\n",
        "    model,\n",
        "    Wiki_test_loader if dataset == \"Wiki\" else PMQA_test_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    len(Wiki_test_dataset) if dataset == \"Wiki\" else len(PMQA_test_dataset),\n",
        "    calculate_statistical_features\n",
        ")\n",
        "\n",
        "print(f'Test loss {test_loss}, accuracy {test_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEsyb7v0bif5",
        "outputId": "8e4a4385-53e7-4e3f-fd45-45a8f4c41e25"
      },
      "outputs": [],
      "source": [
        "def preprocess(texts):\n",
        "    # Tokenize the texts - this can be a single string or a list of strings\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    return inputs\n",
        "\n",
        "def predict(model, texts, device):\n",
        "    model.eval()\n",
        "\n",
        "    inputs = preprocess(texts)\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    return predictions.cpu().numpy()\n",
        "\n",
        "text = \"As ILC2s are elevated in patients with CRSwNP, they may drive nasal polyp formation in CRS. ILC2s are also linked with high tissue and blood eosinophilia and have a potential role in the activation and survival of eosinophils during the Th2 immune response. The association of innate lymphoid cells in CRS provides insights into its pathogenesis.\"\n",
        "single_prediction = predict(model, text, device)\n",
        "print(single_prediction[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
